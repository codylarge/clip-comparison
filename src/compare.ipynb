{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from clip_util import load_custom_clip_model, load_basic_clip_model, get_text_features, get_image_features, compute_similarity, classify_image, classify_v2, CLIPFineTuner\n",
    "from classes import get_candidate_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['model.positional_embedding', 'model.text_projection', 'model.logit_scale', 'model.visual.class_embedding', 'model.visual.positional_embedding', 'model.visual.proj', 'model.visual.conv1.weight', 'model.visual.ln_pre.weight', 'model.visual.ln_pre.bias', 'model.visual.transformer.resblocks.0.attn.in_proj_weight', 'model.visual.transformer.resblocks.0.attn.in_proj_bias', 'model.visual.transformer.resblocks.0.attn.out_proj.weight', 'model.visual.transformer.resblocks.0.attn.out_proj.bias', 'model.visual.transformer.resblocks.0.ln_1.weight', 'model.visual.transformer.resblocks.0.ln_1.bias', 'model.visual.transformer.resblocks.0.mlp.c_fc.weight', 'model.visual.transformer.resblocks.0.mlp.c_fc.bias', 'model.visual.transformer.resblocks.0.mlp.c_proj.weight', 'model.visual.transformer.resblocks.0.mlp.c_proj.bias', 'model.visual.transformer.resblocks.0.ln_2.weight', 'model.visual.transformer.resblocks.0.ln_2.bias', 'model.visual.transformer.resblocks.1.attn.in_proj_weight', 'model.visual.transformer.resblocks.1.attn.in_proj_bias', 'model.visual.transformer.resblocks.1.attn.out_proj.weight', 'model.visual.transformer.resblocks.1.attn.out_proj.bias', 'model.visual.transformer.resblocks.1.ln_1.weight', 'model.visual.transformer.resblocks.1.ln_1.bias', 'model.visual.transformer.resblocks.1.mlp.c_fc.weight', 'model.visual.transformer.resblocks.1.mlp.c_fc.bias', 'model.visual.transformer.resblocks.1.mlp.c_proj.weight', 'model.visual.transformer.resblocks.1.mlp.c_proj.bias', 'model.visual.transformer.resblocks.1.ln_2.weight', 'model.visual.transformer.resblocks.1.ln_2.bias', 'model.visual.transformer.resblocks.2.attn.in_proj_weight', 'model.visual.transformer.resblocks.2.attn.in_proj_bias', 'model.visual.transformer.resblocks.2.attn.out_proj.weight', 'model.visual.transformer.resblocks.2.attn.out_proj.bias', 'model.visual.transformer.resblocks.2.ln_1.weight', 'model.visual.transformer.resblocks.2.ln_1.bias', 'model.visual.transformer.resblocks.2.mlp.c_fc.weight', 'model.visual.transformer.resblocks.2.mlp.c_fc.bias', 'model.visual.transformer.resblocks.2.mlp.c_proj.weight', 'model.visual.transformer.resblocks.2.mlp.c_proj.bias', 'model.visual.transformer.resblocks.2.ln_2.weight', 'model.visual.transformer.resblocks.2.ln_2.bias', 'model.visual.transformer.resblocks.3.attn.in_proj_weight', 'model.visual.transformer.resblocks.3.attn.in_proj_bias', 'model.visual.transformer.resblocks.3.attn.out_proj.weight', 'model.visual.transformer.resblocks.3.attn.out_proj.bias', 'model.visual.transformer.resblocks.3.ln_1.weight', 'model.visual.transformer.resblocks.3.ln_1.bias', 'model.visual.transformer.resblocks.3.mlp.c_fc.weight', 'model.visual.transformer.resblocks.3.mlp.c_fc.bias', 'model.visual.transformer.resblocks.3.mlp.c_proj.weight', 'model.visual.transformer.resblocks.3.mlp.c_proj.bias', 'model.visual.transformer.resblocks.3.ln_2.weight', 'model.visual.transformer.resblocks.3.ln_2.bias', 'model.visual.transformer.resblocks.4.attn.in_proj_weight', 'model.visual.transformer.resblocks.4.attn.in_proj_bias', 'model.visual.transformer.resblocks.4.attn.out_proj.weight', 'model.visual.transformer.resblocks.4.attn.out_proj.bias', 'model.visual.transformer.resblocks.4.ln_1.weight', 'model.visual.transformer.resblocks.4.ln_1.bias', 'model.visual.transformer.resblocks.4.mlp.c_fc.weight', 'model.visual.transformer.resblocks.4.mlp.c_fc.bias', 'model.visual.transformer.resblocks.4.mlp.c_proj.weight', 'model.visual.transformer.resblocks.4.mlp.c_proj.bias', 'model.visual.transformer.resblocks.4.ln_2.weight', 'model.visual.transformer.resblocks.4.ln_2.bias', 'model.visual.transformer.resblocks.5.attn.in_proj_weight', 'model.visual.transformer.resblocks.5.attn.in_proj_bias', 'model.visual.transformer.resblocks.5.attn.out_proj.weight', 'model.visual.transformer.resblocks.5.attn.out_proj.bias', 'model.visual.transformer.resblocks.5.ln_1.weight', 'model.visual.transformer.resblocks.5.ln_1.bias', 'model.visual.transformer.resblocks.5.mlp.c_fc.weight', 'model.visual.transformer.resblocks.5.mlp.c_fc.bias', 'model.visual.transformer.resblocks.5.mlp.c_proj.weight', 'model.visual.transformer.resblocks.5.mlp.c_proj.bias', 'model.visual.transformer.resblocks.5.ln_2.weight', 'model.visual.transformer.resblocks.5.ln_2.bias', 'model.visual.transformer.resblocks.6.attn.in_proj_weight', 'model.visual.transformer.resblocks.6.attn.in_proj_bias', 'model.visual.transformer.resblocks.6.attn.out_proj.weight', 'model.visual.transformer.resblocks.6.attn.out_proj.bias', 'model.visual.transformer.resblocks.6.ln_1.weight', 'model.visual.transformer.resblocks.6.ln_1.bias', 'model.visual.transformer.resblocks.6.mlp.c_fc.weight', 'model.visual.transformer.resblocks.6.mlp.c_fc.bias', 'model.visual.transformer.resblocks.6.mlp.c_proj.weight', 'model.visual.transformer.resblocks.6.mlp.c_proj.bias', 'model.visual.transformer.resblocks.6.ln_2.weight', 'model.visual.transformer.resblocks.6.ln_2.bias', 'model.visual.transformer.resblocks.7.attn.in_proj_weight', 'model.visual.transformer.resblocks.7.attn.in_proj_bias', 'model.visual.transformer.resblocks.7.attn.out_proj.weight', 'model.visual.transformer.resblocks.7.attn.out_proj.bias', 'model.visual.transformer.resblocks.7.ln_1.weight', 'model.visual.transformer.resblocks.7.ln_1.bias', 'model.visual.transformer.resblocks.7.mlp.c_fc.weight', 'model.visual.transformer.resblocks.7.mlp.c_fc.bias', 'model.visual.transformer.resblocks.7.mlp.c_proj.weight', 'model.visual.transformer.resblocks.7.mlp.c_proj.bias', 'model.visual.transformer.resblocks.7.ln_2.weight', 'model.visual.transformer.resblocks.7.ln_2.bias', 'model.visual.transformer.resblocks.8.attn.in_proj_weight', 'model.visual.transformer.resblocks.8.attn.in_proj_bias', 'model.visual.transformer.resblocks.8.attn.out_proj.weight', 'model.visual.transformer.resblocks.8.attn.out_proj.bias', 'model.visual.transformer.resblocks.8.ln_1.weight', 'model.visual.transformer.resblocks.8.ln_1.bias', 'model.visual.transformer.resblocks.8.mlp.c_fc.weight', 'model.visual.transformer.resblocks.8.mlp.c_fc.bias', 'model.visual.transformer.resblocks.8.mlp.c_proj.weight', 'model.visual.transformer.resblocks.8.mlp.c_proj.bias', 'model.visual.transformer.resblocks.8.ln_2.weight', 'model.visual.transformer.resblocks.8.ln_2.bias', 'model.visual.transformer.resblocks.9.attn.in_proj_weight', 'model.visual.transformer.resblocks.9.attn.in_proj_bias', 'model.visual.transformer.resblocks.9.attn.out_proj.weight', 'model.visual.transformer.resblocks.9.attn.out_proj.bias', 'model.visual.transformer.resblocks.9.ln_1.weight', 'model.visual.transformer.resblocks.9.ln_1.bias', 'model.visual.transformer.resblocks.9.mlp.c_fc.weight', 'model.visual.transformer.resblocks.9.mlp.c_fc.bias', 'model.visual.transformer.resblocks.9.mlp.c_proj.weight', 'model.visual.transformer.resblocks.9.mlp.c_proj.bias', 'model.visual.transformer.resblocks.9.ln_2.weight', 'model.visual.transformer.resblocks.9.ln_2.bias', 'model.visual.transformer.resblocks.10.attn.in_proj_weight', 'model.visual.transformer.resblocks.10.attn.in_proj_bias', 'model.visual.transformer.resblocks.10.attn.out_proj.weight', 'model.visual.transformer.resblocks.10.attn.out_proj.bias', 'model.visual.transformer.resblocks.10.ln_1.weight', 'model.visual.transformer.resblocks.10.ln_1.bias', 'model.visual.transformer.resblocks.10.mlp.c_fc.weight', 'model.visual.transformer.resblocks.10.mlp.c_fc.bias', 'model.visual.transformer.resblocks.10.mlp.c_proj.weight', 'model.visual.transformer.resblocks.10.mlp.c_proj.bias', 'model.visual.transformer.resblocks.10.ln_2.weight', 'model.visual.transformer.resblocks.10.ln_2.bias', 'model.visual.transformer.resblocks.11.attn.in_proj_weight', 'model.visual.transformer.resblocks.11.attn.in_proj_bias', 'model.visual.transformer.resblocks.11.attn.out_proj.weight', 'model.visual.transformer.resblocks.11.attn.out_proj.bias', 'model.visual.transformer.resblocks.11.ln_1.weight', 'model.visual.transformer.resblocks.11.ln_1.bias', 'model.visual.transformer.resblocks.11.mlp.c_fc.weight', 'model.visual.transformer.resblocks.11.mlp.c_fc.bias', 'model.visual.transformer.resblocks.11.mlp.c_proj.weight', 'model.visual.transformer.resblocks.11.mlp.c_proj.bias', 'model.visual.transformer.resblocks.11.ln_2.weight', 'model.visual.transformer.resblocks.11.ln_2.bias', 'model.visual.ln_post.weight', 'model.visual.ln_post.bias', 'model.transformer.resblocks.0.attn.in_proj_weight', 'model.transformer.resblocks.0.attn.in_proj_bias', 'model.transformer.resblocks.0.attn.out_proj.weight', 'model.transformer.resblocks.0.attn.out_proj.bias', 'model.transformer.resblocks.0.ln_1.weight', 'model.transformer.resblocks.0.ln_1.bias', 'model.transformer.resblocks.0.mlp.c_fc.weight', 'model.transformer.resblocks.0.mlp.c_fc.bias', 'model.transformer.resblocks.0.mlp.c_proj.weight', 'model.transformer.resblocks.0.mlp.c_proj.bias', 'model.transformer.resblocks.0.ln_2.weight', 'model.transformer.resblocks.0.ln_2.bias', 'model.transformer.resblocks.1.attn.in_proj_weight', 'model.transformer.resblocks.1.attn.in_proj_bias', 'model.transformer.resblocks.1.attn.out_proj.weight', 'model.transformer.resblocks.1.attn.out_proj.bias', 'model.transformer.resblocks.1.ln_1.weight', 'model.transformer.resblocks.1.ln_1.bias', 'model.transformer.resblocks.1.mlp.c_fc.weight', 'model.transformer.resblocks.1.mlp.c_fc.bias', 'model.transformer.resblocks.1.mlp.c_proj.weight', 'model.transformer.resblocks.1.mlp.c_proj.bias', 'model.transformer.resblocks.1.ln_2.weight', 'model.transformer.resblocks.1.ln_2.bias', 'model.transformer.resblocks.2.attn.in_proj_weight', 'model.transformer.resblocks.2.attn.in_proj_bias', 'model.transformer.resblocks.2.attn.out_proj.weight', 'model.transformer.resblocks.2.attn.out_proj.bias', 'model.transformer.resblocks.2.ln_1.weight', 'model.transformer.resblocks.2.ln_1.bias', 'model.transformer.resblocks.2.mlp.c_fc.weight', 'model.transformer.resblocks.2.mlp.c_fc.bias', 'model.transformer.resblocks.2.mlp.c_proj.weight', 'model.transformer.resblocks.2.mlp.c_proj.bias', 'model.transformer.resblocks.2.ln_2.weight', 'model.transformer.resblocks.2.ln_2.bias', 'model.transformer.resblocks.3.attn.in_proj_weight', 'model.transformer.resblocks.3.attn.in_proj_bias', 'model.transformer.resblocks.3.attn.out_proj.weight', 'model.transformer.resblocks.3.attn.out_proj.bias', 'model.transformer.resblocks.3.ln_1.weight', 'model.transformer.resblocks.3.ln_1.bias', 'model.transformer.resblocks.3.mlp.c_fc.weight', 'model.transformer.resblocks.3.mlp.c_fc.bias', 'model.transformer.resblocks.3.mlp.c_proj.weight', 'model.transformer.resblocks.3.mlp.c_proj.bias', 'model.transformer.resblocks.3.ln_2.weight', 'model.transformer.resblocks.3.ln_2.bias', 'model.transformer.resblocks.4.attn.in_proj_weight', 'model.transformer.resblocks.4.attn.in_proj_bias', 'model.transformer.resblocks.4.attn.out_proj.weight', 'model.transformer.resblocks.4.attn.out_proj.bias', 'model.transformer.resblocks.4.ln_1.weight', 'model.transformer.resblocks.4.ln_1.bias', 'model.transformer.resblocks.4.mlp.c_fc.weight', 'model.transformer.resblocks.4.mlp.c_fc.bias', 'model.transformer.resblocks.4.mlp.c_proj.weight', 'model.transformer.resblocks.4.mlp.c_proj.bias', 'model.transformer.resblocks.4.ln_2.weight', 'model.transformer.resblocks.4.ln_2.bias', 'model.transformer.resblocks.5.attn.in_proj_weight', 'model.transformer.resblocks.5.attn.in_proj_bias', 'model.transformer.resblocks.5.attn.out_proj.weight', 'model.transformer.resblocks.5.attn.out_proj.bias', 'model.transformer.resblocks.5.ln_1.weight', 'model.transformer.resblocks.5.ln_1.bias', 'model.transformer.resblocks.5.mlp.c_fc.weight', 'model.transformer.resblocks.5.mlp.c_fc.bias', 'model.transformer.resblocks.5.mlp.c_proj.weight', 'model.transformer.resblocks.5.mlp.c_proj.bias', 'model.transformer.resblocks.5.ln_2.weight', 'model.transformer.resblocks.5.ln_2.bias', 'model.transformer.resblocks.6.attn.in_proj_weight', 'model.transformer.resblocks.6.attn.in_proj_bias', 'model.transformer.resblocks.6.attn.out_proj.weight', 'model.transformer.resblocks.6.attn.out_proj.bias', 'model.transformer.resblocks.6.ln_1.weight', 'model.transformer.resblocks.6.ln_1.bias', 'model.transformer.resblocks.6.mlp.c_fc.weight', 'model.transformer.resblocks.6.mlp.c_fc.bias', 'model.transformer.resblocks.6.mlp.c_proj.weight', 'model.transformer.resblocks.6.mlp.c_proj.bias', 'model.transformer.resblocks.6.ln_2.weight', 'model.transformer.resblocks.6.ln_2.bias', 'model.transformer.resblocks.7.attn.in_proj_weight', 'model.transformer.resblocks.7.attn.in_proj_bias', 'model.transformer.resblocks.7.attn.out_proj.weight', 'model.transformer.resblocks.7.attn.out_proj.bias', 'model.transformer.resblocks.7.ln_1.weight', 'model.transformer.resblocks.7.ln_1.bias', 'model.transformer.resblocks.7.mlp.c_fc.weight', 'model.transformer.resblocks.7.mlp.c_fc.bias', 'model.transformer.resblocks.7.mlp.c_proj.weight', 'model.transformer.resblocks.7.mlp.c_proj.bias', 'model.transformer.resblocks.7.ln_2.weight', 'model.transformer.resblocks.7.ln_2.bias', 'model.transformer.resblocks.8.attn.in_proj_weight', 'model.transformer.resblocks.8.attn.in_proj_bias', 'model.transformer.resblocks.8.attn.out_proj.weight', 'model.transformer.resblocks.8.attn.out_proj.bias', 'model.transformer.resblocks.8.ln_1.weight', 'model.transformer.resblocks.8.ln_1.bias', 'model.transformer.resblocks.8.mlp.c_fc.weight', 'model.transformer.resblocks.8.mlp.c_fc.bias', 'model.transformer.resblocks.8.mlp.c_proj.weight', 'model.transformer.resblocks.8.mlp.c_proj.bias', 'model.transformer.resblocks.8.ln_2.weight', 'model.transformer.resblocks.8.ln_2.bias', 'model.transformer.resblocks.9.attn.in_proj_weight', 'model.transformer.resblocks.9.attn.in_proj_bias', 'model.transformer.resblocks.9.attn.out_proj.weight', 'model.transformer.resblocks.9.attn.out_proj.bias', 'model.transformer.resblocks.9.ln_1.weight', 'model.transformer.resblocks.9.ln_1.bias', 'model.transformer.resblocks.9.mlp.c_fc.weight', 'model.transformer.resblocks.9.mlp.c_fc.bias', 'model.transformer.resblocks.9.mlp.c_proj.weight', 'model.transformer.resblocks.9.mlp.c_proj.bias', 'model.transformer.resblocks.9.ln_2.weight', 'model.transformer.resblocks.9.ln_2.bias', 'model.transformer.resblocks.10.attn.in_proj_weight', 'model.transformer.resblocks.10.attn.in_proj_bias', 'model.transformer.resblocks.10.attn.out_proj.weight', 'model.transformer.resblocks.10.attn.out_proj.bias', 'model.transformer.resblocks.10.ln_1.weight', 'model.transformer.resblocks.10.ln_1.bias', 'model.transformer.resblocks.10.mlp.c_fc.weight', 'model.transformer.resblocks.10.mlp.c_fc.bias', 'model.transformer.resblocks.10.mlp.c_proj.weight', 'model.transformer.resblocks.10.mlp.c_proj.bias', 'model.transformer.resblocks.10.ln_2.weight', 'model.transformer.resblocks.10.ln_2.bias', 'model.transformer.resblocks.11.attn.in_proj_weight', 'model.transformer.resblocks.11.attn.in_proj_bias', 'model.transformer.resblocks.11.attn.out_proj.weight', 'model.transformer.resblocks.11.attn.out_proj.bias', 'model.transformer.resblocks.11.ln_1.weight', 'model.transformer.resblocks.11.ln_1.bias', 'model.transformer.resblocks.11.mlp.c_fc.weight', 'model.transformer.resblocks.11.mlp.c_fc.bias', 'model.transformer.resblocks.11.mlp.c_proj.weight', 'model.transformer.resblocks.11.mlp.c_proj.bias', 'model.transformer.resblocks.11.ln_2.weight', 'model.transformer.resblocks.11.ln_2.bias', 'model.token_embedding.weight', 'model.ln_final.weight', 'model.ln_final.bias', 'classifier.weight', 'classifier.bias'])\n"
     ]
    }
   ],
   "source": [
    "basic_model, basic_preprocess, basic_device = load_basic_clip_model()\n",
    "custom_model, custom_preprocess, custom_device = load_custom_clip_model(\"../clip_finetuned.pth\", 13)\n",
    "\n",
    "print(custom_model.state_dict().keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corn_dataset = {\n",
    "    \"common_rust\": \"../dataset/corn/common_rust\",\n",
    "    \"gray_leaf_spot\": \"../dataset/corn/gray_leaf_spot\",\n",
    "    \"healthy\": \"../dataset/corn/healthy\",\n",
    "    \"northern_leaf_blight\": \"../dataset/corn/northern_leaf_blight\",\n",
    "}\n",
    "\n",
    "potato_dataset = {\n",
    "    \"early_blight\": \"../dataset/potato/early_blight\",\n",
    "    \"late_blight\": \"../dataset/potato/late_blight\",\n",
    "}\n",
    "\n",
    "rice_dataset = {\n",
    "    \"brown_spot\": \"../dataset/rice/brown_spot\",\n",
    "    \"healthy\": \"../dataset/rice/healthy\",\n",
    "    \"leaf_blast\": \"../dataset/rice/leaf_blast\",\n",
    "    \"neck_blast\": \"../dataset/rice/neck_blast\",\n",
    "}\n",
    "\n",
    "wheat_dataset = {\n",
    "    \"healthy\": \"../dataset/wheat/healthy\",\n",
    "    \"brown_rust\": \"../dataset/wheat/brown_rust\",\n",
    "    \"yellow_rust\": \"../dataset/wheat/yellow_rust\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_image_from_folder(folder_path):\n",
    "    images = [os.path.join(folder_path, img) for img in os.listdir(folder_path)]\n",
    "    if images:\n",
    "        return random.choice(images)\n",
    "    else:\n",
    "        raise ValueError(f\"No images found in folder {folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_comparison(image_path, basic_classified_class, basic_similarity_score, custom_classified_class, custom_similarity_score):\n",
    "    # Display the image\n",
    "    image = Image.open(image_path)\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(8, 3))  # Create a figure with 1 row and 2 columns\n",
    "    \n",
    "    # Display the image classified by basic CLIP\n",
    "    axs[0].imshow(image)\n",
    "    axs[0].set_title(f\"Basic CLIP: {basic_classified_class}\\nSimilarity Score: {basic_similarity_score:.4f}\")\n",
    "    axs[0].axis('off')\n",
    "    \n",
    "    # Display the image classified by custom CLIP\n",
    "    axs[1].imshow(image)\n",
    "    axs[1].set_title(f\"Custom CLIP: {custom_classified_class}\\nSimilarity Score: {custom_similarity_score:.4f}\")\n",
    "    axs[1].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying images from Corn: Common rust...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 12 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m image_path \u001b[38;5;241m=\u001b[39m get_random_image_from_folder(folder_path)\n\u001b[0;32m     10\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_path)\n\u001b[1;32m---> 11\u001b[0m basic_classified_class, basic_best_caption, basic_similarity_score \u001b[38;5;241m=\u001b[39m \u001b[43mclassify_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasic_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasic_preprocess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasic_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m custom_classified_class, custom_best_caption, custom_similarity_score \u001b[38;5;241m=\u001b[39m classify_image(image_path, custom_model, custom_preprocess, custom_device)\n\u001b[0;32m     13\u001b[0m display_comparison(image_path, basic_classified_class, basic_similarity_score, custom_classified_class, custom_similarity_score)\n",
      "File \u001b[1;32mc:\\My Stuff\\Programming\\Python Projects\\clip-comparison\\src\\clip_util.py:138\u001b[0m, in \u001b[0;36mclassify_image\u001b[1;34m(image_path, model, preprocess, device)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Extract the class name from the best caption\u001b[39;00m\n\u001b[0;32m    136\u001b[0m classified_class \u001b[38;5;241m=\u001b[39m best_caption\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m classified_class, best_caption, \u001b[43msimilarity\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbest_match_index\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mIndexError\u001b[0m: index 12 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "datasets = [corn_dataset, potato_dataset, rice_dataset, wheat_dataset]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for class_name, folder_path in dataset.items():\n",
    "        # Extract correct class from directory path\n",
    "        parts = folder_path.strip(\"/\").split(\"/\")\n",
    "        actual_class = f\"{parts[-2].capitalize()}: {parts[-1].replace('_', ' ').capitalize()}\"\n",
    "        print(f\"Classifying images from {actual_class}...\")\n",
    "        image_path = get_random_image_from_folder(folder_path)\n",
    "        image = Image.open(image_path)\n",
    "        basic_classified_class, basic_best_caption, basic_similarity_score = classify_image(image_path, basic_model, basic_preprocess, basic_device)\n",
    "        custom_prediction, custom_similarity = classify_v2(image_path, custom_model, custom_preprocess, custom_device)\n",
    "        #custom_classified_class, custom_best_caption, custom_similarity_score = classify_image(image_path, custom_model, custom_preprocess, custom_device)\n",
    "        display_comparison(image_path, basic_classified_class, basic_similarity_score, custom_classified_class, custom_similarity_score)\n",
    "        plt.show()\n",
    "\n",
    "#image_path = get_random_image_from_folder(corn_dataset[\"common_rust\"])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
